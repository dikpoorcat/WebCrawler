import requests
from bs4 import BeautifulSoup
import os
import time
import re
from selenium import webdriver
from selenium.common import TimeoutException, WebDriverException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# --- é…ç½®åŒºåŸŸ ---
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
base_url = 'http://www.oushenwenji.net/'
forum_url = 'http://www.oushenwenji.net/forum.php'
root_save_dir = 'æ¬§ç¥æ–‡é›†ä¸‹è½½'
# ----------------

if not os.path.exists(root_save_dir):
    os.makedirs(root_save_dir)


def get_soup(url):
    """é€šç”¨è¯·æ±‚å‡½æ•°ï¼Œå¢åŠ æ›´å¤šè¯·æ±‚å¤´å’Œå¤„ç†"""
    try:
        # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Referer': 'http://www.oushenwenji.net/'
        }

        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'

        # æ£€æŸ¥å“åº”çŠ¶æ€ç å’Œå†…å®¹é•¿åº¦
        if response.status_code != 200:
            print(f"      HTTPçŠ¶æ€ç é”™è¯¯: {response.status_code}")
            return None

        if len(response.text) < 100:
            print(f"      å“åº”å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½è¢«é‡å®šå‘æˆ–æ‹’ç»è®¿é—®")
            print(f"      å“åº”é¢„è§ˆ: {response.text[:200]}")
            return None

        return BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        print(f"      [è¯·æ±‚é”™è¯¯] {url}: {e}")
        return None


def get_forum_info():
    """
    è·å–æ¿å—ä¿¡æ¯
    è¿”å›æ ¼å¼: [{'name': 'æ¿å—å', 'url': 'é“¾æ¥'}, ...]
    """
    soup = get_soup(forum_url)
    if not soup:
        return []

    forums = []
    # æŸ¥æ‰¾æ¿å—é“¾æ¥
    for link in soup.find_all('a', href=True):
        href = link['href']
        # åŒ¹é… forum-æ•°å­—-1.html æˆ–ç±»ä¼¼çš„æ¿å—é“¾æ¥è§„åˆ™
        if ('forum' in href and '.html' in href) or ('mod=forumdisplay' in href):
            if not href.startswith('http'):
                full_url = base_url + href if not href.startswith('/') else base_url.rstrip('/') + href
            else:
                full_url = href

            # æå–æ¿å—åç§°
            name = link.text.strip()

            # è¿‡æ»¤æ— æ•ˆæˆ–é‡å¤çš„æ¿å—ï¼ˆæ ¹æ®åå­—æˆ–é“¾æ¥å»é‡ï¼‰
            # è¿™é‡Œçš„åˆ¤æ–­é€»è¾‘æ˜¯ï¼šåå­—ä¸ä¸ºç©ºï¼Œä¸”é“¾æ¥æœªè¢«æ·»åŠ è¿‡
            if name and not any(f['url'] == full_url for f in forums):
                forums.append({'name': name, 'url': full_url})
                print(f"å‘ç°æ¿å—: {name} -> {full_url}")

    return forums


def get_post_links(forum_url):
    """
    è·å–æŸæ¿å—ä¸‹çš„æ‰€æœ‰å¸–å­é“¾æ¥
    """
    post_links = []

    # æå– fid
    match = re.search(r'forum-(\d+)', forum_url)
    if not match:
        print(f"  æ— æ³•è§£æfidï¼Œè·³è¿‡ç¿»é¡µé€»è¾‘: {forum_url}")
        return []

    fid = match.group(1)
    page = 1
    max_pages = 50  # å®‰å…¨é˜ˆå€¼

    while page <= max_pages:
        current_url = f"{base_url}forum-{fid}-{page}.html"
        # print(f"  æ­£åœ¨åˆ†æç¬¬ {page} é¡µ: {current_url}") # è°ƒè¯•æ—¶å¯å¼€å¯

        soup = get_soup(current_url)
        if not soup:
            break

        found_new = 0
        # æŸ¥æ‰¾å¸–å­é“¾æ¥ (é€šå¸¸ class="xst" æˆ–åœ¨ id="threadlist" ä¸­)
        # è¿™é‡Œä½¿ç”¨æ¯”è¾ƒé€šç”¨çš„åŒ…å« thread çš„é“¾æ¥æŸ¥æ‰¾
        for link in soup.find_all('a', href=True):
            href = link['href']
            # æ’é™¤å›å¤é“¾æ¥ï¼Œé€šå¸¸å¸–å­é“¾æ¥æ˜¯ thread-ID-1-1.html
            if 'thread' in href and '.html' in href and 'unapproved' not in href:
                if not href.startswith('http'):
                    full_url = base_url + href if not href.startswith('/') else base_url.rstrip('/') + href
                else:
                    full_url = href

                if full_url not in post_links:
                    post_links.append(full_url)
                    found_new += 1

        if found_new == 0:
            # å¦‚æœè¿™ä¸€é¡µæ²¡æœ‰ä»»ä½•æ–°å¸–å­ï¼Œé€šå¸¸æ„å‘³ç€å·²ç»è¶…å‡ºäº†æœ€å¤§é¡µæ•°
            break

        # æ£€æŸ¥æ˜¯å¦æœ‰"ä¸‹ä¸€é¡µ" (Discuz é€šç”¨ class 'nxt')
        if not soup.find('a', class_='nxt') and page > 1:
            break

        page += 1
        time.sleep(0.5)

    return post_links


def get_soup_selenium(url):
    """
    ä½¿ç”¨Seleniumè·å–é¡µé¢ï¼Œå¯æ‰§è¡ŒJavaScript
    å¢åŠ è§†é¢‘èµ„æºå¤„ç†ä¼˜åŒ–
    """
    # åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨ï¼Œä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ˆä¸å¼¹å‡ºçª—å£ï¼‰
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # æ— å¤´æ¨¡å¼ï¼Œåå°è¿è¡Œ
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    # å…³é”®ï¼šç¦æ­¢åŠ è½½å›¾ç‰‡å’Œè§†é¢‘ï¼Œå¤§å¹…æé«˜åŠ è½½é€Ÿåº¦
    prefs = {
        'profile.default_content_setting_values': {
            'images': 2,  # 1:å…è®¸, 2:é˜»æ­¢
            'plugins': 2,  # é˜»æ­¢Flash
            'popups': 2,  # é˜»æ­¢å¼¹çª—
            'geolocation': 2,  # é˜»æ­¢åœ°ç†ä½ç½®
            'notifications': 2,  # é˜»æ­¢é€šçŸ¥
            'media_stream': 2,  # é˜»æ­¢åª’ä½“æµ
            'media_stream_mic': 2,  # é˜»æ­¢éº¦å…‹é£
            'media_stream_camera': 2,  # é˜»æ­¢æ‘„åƒå¤´
            'automatic_downloads': 2,  # é˜»æ­¢è‡ªåŠ¨ä¸‹è½½
        }
    }
    options.add_experimental_option('prefs', prefs)

    # è®¾ç½®User-Agent
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

    # æ·»åŠ é¢å¤–çš„æ€§èƒ½ä¼˜åŒ–å‚æ•°
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-logging')
    options.add_argument('--disable-notifications')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-features=IsolateOrigins,site-per-process')

    driver = None
    try:
        driver = webdriver.Chrome(options=options)

        # è®¾ç½®è¶…æ—¶ç­–ç•¥
        driver.set_page_load_timeout(15)  # é¡µé¢åŠ è½½è¶…æ—¶15ç§’
        driver.set_script_timeout(10)  # è„šæœ¬æ‰§è¡Œè¶…æ—¶10ç§’

        # ä½¿ç”¨JavaScriptç¦ç”¨è§†é¢‘é¢„åŠ è½½
        driver.execute_cdp_cmd('Network.setBlockedURLs', {
            "urls": [
                "*.mp4", "*.webm", "*.ogg", "*.avi", "*.mov", "*.flv",
                "*.m3u8", "*.mpd", "*.m4v", "*video*", "*stream*"
            ]
        })

        driver.get(url)

        # æ›´æ™ºèƒ½çš„ç­‰å¾…ç­–ç•¥ï¼šç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆ
        try:
            # ç­‰å¾…bodyå…ƒç´ åŠ è½½
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # é¢å¤–ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©JavaScriptæ‰§è¡Œ
            time.sleep(1)

        except TimeoutException:
            # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿå¯èƒ½å·²ç»æœ‰éƒ¨åˆ†å†…å®¹äº†
            print("      é¡µé¢åŠ è½½å¯èƒ½è¾ƒæ…¢ï¼Œä½†å·²è·å–éƒ¨åˆ†å†…å®¹")

        # è·å–å½“å‰URL
        real_url = driver.current_url
        print(f"      è·å–åˆ°çœŸå®URL: {real_url}")

        # è·å–é¡µé¢æºä»£ç 
        html = driver.page_source

        # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«è§†é¢‘ç›¸å…³é”™è¯¯
        if "è§†é¢‘" in html and ("æ— æ³•æ’­æ”¾" in html or "åŠ è½½å¤±è´¥" in html or "å·²å¤±æ•ˆ" in html):
            print("      æ£€æµ‹åˆ°è§†é¢‘å·²å¤±æ•ˆï¼Œè·³è¿‡è§†é¢‘èµ„æº")

        return BeautifulSoup(html, 'html.parser'), real_url

    except TimeoutException as e:
        print(f"      [Seleniumè¶…æ—¶] {url}: é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•è·å–å·²æœ‰å†…å®¹")
        if driver:
            try:
                # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿå°è¯•è·å–å½“å‰é¡µé¢å†…å®¹
                html = driver.page_source
                real_url = driver.current_url
                if html and len(html) > 1000:  # å¦‚æœæœ‰è¶³å¤Ÿçš„å†…å®¹
                    return BeautifulSoup(html, 'html.parser'), real_url
            except:
                pass
        return None, None

    except WebDriverException as e:
        print(f"      [Seleniumé”™è¯¯] {url}: {str(e)[:100]}...")
        return None, None

    except Exception as e:
        print(f"      [Seleniumå¼‚å¸¸] {url}: {e}")
        return None, None

    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass


def is_video_post(url, soup):
    """
    åˆ¤æ–­æ˜¯å¦æ˜¯è§†é¢‘å¸–å­
    """
    if not soup:
        return False

    # æ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦åŒ…å«è§†é¢‘å…³é”®è¯
    page_text = str(soup).lower()
    video_keywords = ['video', 'mp4', 'flv', 'avi', 'mov', 'wmv', 'mkv', 'webm']

    for keyword in video_keywords:
        if keyword in page_text:
            return True

    # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘æ’­æ”¾å™¨ç›¸å…³æ ‡ç­¾
    video_tags = soup.find_all(['video', 'iframe', 'embed', 'object'])
    if len(video_tags) > 0:
        return True

    # æ£€æŸ¥é¡µé¢æ ‡é¢˜æ˜¯å¦åŒ…å«è§†é¢‘ç›¸å…³è¯æ±‡
    title_tag = soup.find('title')
    if title_tag:
        title = title_tag.text.lower()
        if any(word in title for word in ['è§†é¢‘', 'video', 'å½•åƒ', 'å½•æ’­']):
            return True

    return False


def download_post(post_url, save_folder):
    """
    ä¸‹è½½å¸–å­å†…å®¹å¹¶ä¿å­˜ä¸ºç²¾ç¾çš„HTML
    ä¼˜åŒ–è§†é¢‘å¸–å­å¤„ç†
    """
    print(f"      æ­£åœ¨ä¸‹è½½: {post_url}")

    # å…ˆå°è¯•æ™®é€šè¯·æ±‚ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯è§†é¢‘å¸–å­
    soup_normal = get_soup(post_url)
    is_video = is_video_post(post_url, soup_normal)

    if is_video:
        print(f"      æ£€æµ‹åˆ°è§†é¢‘å¸–å­ï¼Œè·³è¿‡Seleniumç›´æ¥å¤„ç†")
        soup = soup_normal
        real_url = post_url
    else:
        # éè§†é¢‘å¸–å­ä½¿ç”¨Seleniumè·å–
        soup, real_url = get_soup_selenium(post_url)

        # å¦‚æœSeleniumè·å–å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šè¯·æ±‚
        if not soup:
            print(f"      Seleniumè·å–å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šè¯·æ±‚...")
            soup = get_soup(post_url)
            real_url = post_url

    if not soup:
        print(f"      æ— æ³•è·å–é¡µé¢ï¼Œè·³è¿‡: {post_url}")
        return False

    # å¦‚æœæ˜¯è§†é¢‘å¸–å­ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°
    if is_video:
        print(f"      æ³¨æ„ï¼šæ­¤å¸–åŒ…å«è§†é¢‘å†…å®¹ï¼Œè§†é¢‘å¯èƒ½å·²å¤±æ•ˆ")

    # æå–æ ‡é¢˜
    title = "æ— æ ‡é¢˜"
    title_tag = soup.find('title')
    if title_tag:
        title = title_tag.text.strip()
        # æ¸…ç†æ ‡é¢˜ä¸­çš„Discuzæ ‡è¯†
        if ' - Powered by Discuz!' in title:
            title = title.replace(' - Powered by Discuz!', '')
        if ' - ' in title and len(title.split(' - ')) > 1:
            title = title.split(' - ')[0]
        print(f"      æå–åˆ°æ ‡é¢˜: {title[:50]}...")
    else:
        h1_tag = soup.find('h1')
        if h1_tag:
            title = h1_tag.text.strip()
            print(f"      é€šè¿‡h1æå–æ ‡é¢˜: {title[:50]}...")
        else:
            match = re.search(r'thread-(\d+)', post_url)
            if match:
                title = f"å¸–å­_{match.group(1)}"
                print(f"      ä½¿ç”¨URLä½œä¸ºæ ‡é¢˜: {title}")

    # æå–å‘å¸ƒæ—¶é—´
    publish_time = "0000-00-00"
    date_patterns = [
        r'å‘è¡¨äº (\d{4}-\d{1,2}-\d{1,2})',
        r'(\d{4}-\d{1,2}-\d{1,2})',
        r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
        r'(\d{4}/\d{1,2}/\d{1,2})'
    ]

    page_text = str(soup)
    for pattern in date_patterns:
        date_match = re.search(pattern, page_text)
        if date_match:
            try:
                nums = re.findall(r'\d+', date_match.group())
                if len(nums) >= 3:
                    year, month, day = nums[0], nums[1], nums[2]
                    publish_time = f"{int(year)}-{int(month):02d}-{int(day):02d}"
                    print(f"      æ‰¾åˆ°æ—¥æœŸ: {publish_time}")
                    break
            except:
                continue

    # æå–å†…å®¹
    content_html = ""
    content_divs = soup.find_all(['div', 'td'], class_=['t_f', 'postmessage'])

    if content_divs:
        print(f"      æ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œæ•°é‡: {len(content_divs)}")
        main_content = content_divs[0]

        # å¤„ç†å›¾ç‰‡
        for img in main_content.find_all('img'):
            src = img.get('src') or img.get('file') or img.get('zoomfile')
            if src:
                if not src.startswith('http'):
                    if src.startswith('/'):
                        src = base_url.rstrip('/') + src
                    else:
                        src = base_url + src
                img['src'] = src
                img['style'] = "max-width: 100%; height: auto; margin: 10px 0;"

        content_html = str(main_content)
    else:
        # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„å†…å®¹åŒºåŸŸ
        for elem in soup.find_all(id=re.compile('postmessage_')):
            print(f"      é€šè¿‡IDæ‰¾åˆ°å†…å®¹: {elem.get('id')}")
            content_html = str(elem)
            break

        if not content_html:
            for table in soup.find_all('table'):
                if 'post' in str(table.get('class', '')).lower():
                    print(f"      é€šè¿‡è¡¨æ ¼æ‰¾åˆ°å†…å®¹")
                    content_html = str(table)
                    break

        if not content_html:
            for div in soup.find_all('div'):
                text = div.get_text(strip=True)
                if len(text) > 200:
                    print(f"      é€šè¿‡é•¿æ–‡æœ¬divæ‰¾åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(text)}")
                    content_html = str(div)
                    break

        if not content_html:
            print(f"      ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šæå–é¡µé¢ä¸»è¦å†…å®¹")
            body = soup.find('body')
            if body:
                for elem in body.find_all(['script', 'style', 'iframe', 'video', 'audio']):
                    elem.decompose()
                content_html = str(body)
            else:
                content_html = f"<p>æœªèƒ½æå–å†…å®¹</p><p>åŸå§‹URL: <a href='{post_url}'>{post_url}</a></p>"

    # å¦‚æœæ˜¯è§†é¢‘å¸–å­ï¼Œåœ¨å†…å®¹å‰æ·»åŠ æç¤º
    if is_video:
        video_note = """
        <div class="video-note" style="background-color: #fff3cd; border: 1px solid #ffeaa7; 
                border-radius: 5px; padding: 15px; margin-bottom: 20px;">
            <strong>ğŸ“¹ è§†é¢‘å¸–å­æç¤ºï¼š</strong><br>
            æ­¤å¸–å­åŒ…å«è§†é¢‘å†…å®¹ï¼ŒåŸå§‹è§†é¢‘å¯èƒ½å·²å¤±æ•ˆæˆ–æ— æ³•æ’­æ”¾ã€‚<br>
            ä»¥ä¸‹æ˜¯å¸–å­çš„æ–‡æœ¬å†…å®¹å’Œå…¶ä»–ç›¸å…³ä¿¡æ¯ï¼š
        </div>
        """
        content_html = video_note + content_html

    # æ„å»ºæ–‡ä»¶å
    safe_title = re.sub(r'[\\/*?:"<>|]', '', title).strip()
    if not safe_title or len(safe_title) < 2:
        match = re.search(r'thread-(\d+)', post_url)
        if match:
            safe_title = f"thread_{match.group(1)}"
        else:
            safe_title = f"post_{hash(post_url) % 10000}"

    # å¦‚æœæ˜¯è§†é¢‘å¸–å­ï¼Œåœ¨æ–‡ä»¶åä¸­æ ‡è®°
    if is_video:
        safe_title = f"[è§†é¢‘]{safe_title}"

    if len(safe_title) > 100:
        safe_title = safe_title[:100]

    filename = f"{publish_time}_{safe_title}.html"
    filepath = os.path.join(save_folder, filename)

    # ç”Ÿæˆå®Œæ•´çš„HTMLé¡µé¢
    html_template = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <title>{title}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
            margin: 0;
        }}
        .container {{
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1 {{
            font-size: 22px;
            color: #2c3e50;
            margin-bottom: 15px;
            padding-bottom: 15px;
            border-bottom: 1px solid #eee;
        }}
        .meta {{
            color: #666;
            font-size: 14px;
            margin-bottom: 25px;
        }}
        .content {{
            font-size: 16px;
        }}
        .content img {{
            max-width: 100%;
            height: auto;
            margin: 10px 0;
        }}
        .content p {{
            margin-bottom: 15px;
        }}
        .original-link {{
            margin-top: 30px;
            padding-top: 15px;
            border-top: 1px solid #eee;
            font-size: 14px;
            color: #888;
        }}
        .video-note {{
            background-color: #fff3cd;
            border: 1px solid #ffeaa7;
            border-radius: 5px;
            padding: 15px;
            margin-bottom: 20px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>{title}</h1>
        <div class="meta">
            <strong>å‘å¸ƒæ—¥æœŸ:</strong> {publish_time} <br>
            <strong>åŸæ–‡é“¾æ¥:</strong> <a href="{real_url}" target="_blank">{real_url}</a>
        </div>
        <div class="content">
            {content_html}
        </div>
        <div class="original-link">
            æœ¬æ–‡æ¡£ç”±æ¬§ç¥æ–‡é›†ä¸‹è½½å™¨ç”Ÿæˆäº {time.strftime('%Y-%m-%d %H:%M:%S')}
        </div>
    </div>
</body>
</html>"""

    # ä¿å­˜æ–‡ä»¶
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_template)
        print(f"      âœ“ ä¿å­˜æˆåŠŸ: {filename}")
        return True
    except Exception as e:
        print(f"      âœ— ä¿å­˜å¤±è´¥: {e}")
        # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ–‡ä»¶å
        try:
            simple_filename = f"{publish_time}_post_{hash(post_url) % 10000}.html"
            simple_filepath = os.path.join(save_folder, simple_filename)
            with open(simple_filepath, 'w', encoding='utf-8') as f:
                f.write(html_template)
            print(f"      âœ“ ä½¿ç”¨ç®€åŒ–æ–‡ä»¶åä¿å­˜æˆåŠŸ: {simple_filename}")
            return True
        except:
            return False

def main():
    print("=== æ¬§ç¥æ–‡é›†ä¸‹è½½å™¨ (åˆ†ç±»ç‰ˆ) ===")

    # 1. è·å–æ‰€æœ‰æ¿å—
    print("\næ­£åœ¨è·å–æ¿å—åˆ—è¡¨...")
    forums = get_forum_info()

    if not forums:
        print("æœªæ‰¾åˆ°ä»»ä½•æ¿å—ï¼Œç¨‹åºç»“æŸã€‚")
        return

    print(f"å…±æ‰¾åˆ° {len(forums)} ä¸ªæ¿å—ã€‚")

    # 2. éå†æ¯ä¸ªæ¿å—
    for forum in forums:
        forum_name = re.sub(r'[\\/*?:"<>|]', '', forum['name'])
        forum_url = forum['url']

        # ä¸ºè¯¥æ¿å—åˆ›å»ºç‹¬ç«‹æ–‡ä»¶å¤¹
        current_save_dir = os.path.join(root_save_dir, forum_name)
        if not os.path.exists(current_save_dir):
            os.makedirs(current_save_dir)

        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹å¤„ç†æ¿å—: ã€{forum_name}ã€‘")
        print(f"ç›®æ ‡æ–‡ä»¶å¤¹: {current_save_dir}")
        print(f"æ¿å—URL: {forum_url}")

        # 3. è·å–è¯¥æ¿å—ä¸‹çš„å¸–å­
        post_links = get_post_links(forum_url)
        # å»é‡
        post_links = list(set(post_links))
        print(f"å…±å‘ç° {len(post_links)} ç¯‡æ–‡ç« ")

        print("å¼€å§‹ä¸‹è½½...")

        # 4. ä¸‹è½½è¯¥æ¿å—çš„æ–‡ç« ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        success_count = 0
        for i, link in enumerate(post_links, 1):
            print(f"\n[{i}/{len(post_links)}] ", end="")

            # å°è¯•3æ¬¡
            for retry in range(3):
                if retry > 0:
                    print(f"      ç¬¬{retry + 1}æ¬¡é‡è¯•...")
                    time.sleep(2)  # é‡è¯•å‰ç­‰å¾…

                if download_post(link, current_save_dir):
                    success_count += 1
                    break
                else:
                    if retry == 2:
                        print(f"      æ”¾å¼ƒä¸‹è½½: {link}")

            # è¯·æ±‚é—´éš”ï¼Œé¿å…è¢«å°
            time.sleep(1)

        print(f"\næ¿å— ã€{forum_name}ã€‘ å¤„ç†å®Œæˆ")
        print(f"æˆåŠŸä¸‹è½½: {success_count}/{len(post_links)} ç¯‡æ–‡ç« ")
        print(f"{'=' * 60}")

    print(f"\nå…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")
    print(f"æ‰€æœ‰æ–‡ä»¶å·²æŒ‰æ¿å—åˆ†ç±»ä¿å­˜åœ¨ '{root_save_dir}' ä¸­ã€‚")


if __name__ == "__main__":
    main()